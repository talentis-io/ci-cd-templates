name: Run AI Analysis

on:
  workflow_call:
    inputs:
      repo_name:
        required: true
        type: string
      branch_name:
        required: true
        type: string
      sonar_project_key:
        required: false
        type: string
        default: ""
      max_files_to_analyze:
        required: false
        type: number
        default: 15
      max_lines_per_file:
        required: false
        type: number
        default: 300
    outputs:
      score:
        description: "AI analysis score"
        value: ${{ jobs.ai-analysis.outputs.score }}
      analysis_html:
        description: "AI analysis HTML report"
        value: ${{ jobs.ai-analysis.outputs.analysis_html }}
      percentage_advancement:
        description: "Percentage of advancement based on test document"
        value: ${{ jobs.ai-analysis.outputs.percentage_advancement }}

jobs:
  ai-analysis:
    runs-on: ubuntu-latest
    outputs:
      score: ${{ steps.ai-analysis.outputs.score }}
      analysis_html: ${{ steps.ai-analysis.outputs.analysis_html }}
      percentage_advancement: ${{ steps.ai-analysis.outputs.percentage_advancement }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          repository: ${{ github.repository_owner }}/${{ inputs.repo_name }}
          ref: ${{ inputs.branch_name }}
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Get SonarCloud metrics
        id: sonar-metrics
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_PROJECT_KEY: ${{ inputs.sonar_project_key != '' && inputs.sonar_project_key || format('talentis-io_{0}', inputs.repo_name) }}
        run: |
          # Wait a moment for SonarCloud analysis to complete
          sleep 10
          
          # Get project measures from SonarCloud
          RESPONSE=$(curl -s -u $SONAR_TOKEN: \
            "https://sonarcloud.io/api/measures/component?component=$SONAR_PROJECT_KEY&metricKeys=bugs,vulnerabilities,security_hotspots,code_smells,coverage,duplicated_lines_density,ncloc,sqale_index,reliability_rating,security_rating,sqale_rating")
          
          echo "sonar_response<<EOF" >> $GITHUB_OUTPUT
          echo "$RESPONSE" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          # Extract key metrics
          BUGS=$(echo "$RESPONSE" | jq -r '.component.measures[] | select(.metric == "bugs") | .value // "0"')
          VULNERABILITIES=$(echo "$RESPONSE" | jq -r '.component.measures[] | select(.metric == "vulnerabilities") | .value // "0"')
          CODE_SMELLS=$(echo "$RESPONSE" | jq -r '.component.measures[] | select(.metric == "code_smells") | .value // "0"')
          COVERAGE=$(echo "$RESPONSE" | jq -r '.component.measures[] | select(.metric == "coverage") | .value // "0"')
          LINES_OF_CODE=$(echo "$RESPONSE" | jq -r '.component.measures[] | select(.metric == "ncloc") | .value // "0"')
          
          echo "bugs=$BUGS" >> $GITHUB_OUTPUT
          echo "vulnerabilities=$VULNERABILITIES" >> $GITHUB_OUTPUT
          echo "code_smells=$CODE_SMELLS" >> $GITHUB_OUTPUT
          echo "coverage=$COVERAGE" >> $GITHUB_OUTPUT
          echo "lines_of_code=$LINES_OF_CODE" >> $GITHUB_OUTPUT

      - name: Extract and Prepare Code Samples
        id: code-extraction
        env:
          MAX_FILES: ${{ inputs.max_files_to_analyze }}
          MAX_LINES: ${{ inputs.max_lines_per_file }}
        run: |
          echo "Extracting code samples for AI analysis..."
          
          # Create output file
          CODE_SAMPLES_FILE="/tmp/code_samples.txt"
          > "$CODE_SAMPLES_FILE"
          
          # Find relevant source files (prioritize main source files, exclude node_modules, build, dist, etc.)
          FILES=$(find . -type f \
            \( -name "*.js" -o -name "*.jsx" -o -name "*.ts" -o -name "*.tsx" -o -name "*.py" -o -name "*.java" -o -name "*.go" -o -name "*.rb" -o -name "*.php" -o -name "*.cs" -o -name "*.cpp" -o -name "*.c" -o -name "*.h" \) \
            -not -path "*/node_modules/*" \
            -not -path "*/dist/*" \
            -not -path "*/build/*" \
            -not -path "*/.next/*" \
            -not -path "*/coverage/*" \
            -not -path "*/__pycache__/*" \
            -not -path "*/vendor/*" \
            -not -path "*/.git/*" \
            | head -n "$MAX_FILES")
          
          FILE_COUNT=0
          TOTAL_LINES=0
          
          echo "=== CODE SAMPLES FOR AI ANALYSIS ===" >> "$CODE_SAMPLES_FILE"
          echo "" >> "$CODE_SAMPLES_FILE"
          
          for file in $FILES; do
            if [ -f "$file" ]; then
              FILE_COUNT=$((FILE_COUNT + 1))
              LINE_COUNT=$(wc -l < "$file")
              
              echo "--- FILE: $file (Total lines: $LINE_COUNT) ---" >> "$CODE_SAMPLES_FILE"
              
              # Limit lines per file to avoid token overflow
              if [ "$LINE_COUNT" -gt "$MAX_LINES" ]; then
                head -n "$MAX_LINES" "$file" >> "$CODE_SAMPLES_FILE"
                echo "" >> "$CODE_SAMPLES_FILE"
                echo "... (truncated, showing first $MAX_LINES of $LINE_COUNT lines)" >> "$CODE_SAMPLES_FILE"
                TOTAL_LINES=$((TOTAL_LINES + MAX_LINES))
              else
                cat "$file" >> "$CODE_SAMPLES_FILE"
                TOTAL_LINES=$((TOTAL_LINES + LINE_COUNT))
              fi
              
              echo "" >> "$CODE_SAMPLES_FILE"
              echo "" >> "$CODE_SAMPLES_FILE"
            fi
          done
          
          echo "=== END OF CODE SAMPLES ===" >> "$CODE_SAMPLES_FILE"
          
          echo "Extracted $FILE_COUNT files with approximately $TOTAL_LINES lines of code"
          
          # Save to output
          echo "code_samples<<EOF" >> $GITHUB_OUTPUT
          cat "$CODE_SAMPLES_FILE" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          echo "file_count=$FILE_COUNT" >> $GITHUB_OUTPUT
          echo "total_lines=$TOTAL_LINES" >> $GITHUB_OUTPUT

      - name: Prepare project context
        id: project-context
        run: |
          # Get project structure
          PROJECT_STRUCTURE=$(find . -type f \( -name "*.js" -o -name "*.jsx" -o -name "*.ts" -o -name "*.tsx" -o -name "*.json" -o -name "package.json" -o -name "*.md" \) \
            -not -path "*/node_modules/*" \
            -not -path "*/dist/*" \
            -not -path "*/build/*" | head -30)
          
          # Get package.json content if exists
          PACKAGE_CONTENT=""
          if [ -f "package.json" ]; then
            PACKAGE_CONTENT=$(cat package.json)
          fi
          
          # Get README content if exists
          README_CONTENT=""
          if [ -f "README.md" ]; then
            README_CONTENT=$(head -50 README.md)
          fi
          
          echo "project_structure<<EOF" >> $GITHUB_OUTPUT
          echo "$PROJECT_STRUCTURE" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          echo "package_content<<EOF" >> $GITHUB_OUTPUT
          echo "$PACKAGE_CONTENT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          
          echo "readme_content<<EOF" >> $GITHUB_OUTPUT
          echo "$README_CONTENT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Get test attachment document
        id: test-attachment
        env:
          WEBHOOK_URL: ${{ secrets.WEBHOOK_URL }}
          REPO_NAME: ${{ inputs.repo_name }}
        run: |
          # Call the webhook API to get test attachment text
          if [ -z "$WEBHOOK_URL" ]; then
            echo "WARNING: WEBHOOK_URL not set, skipping test attachment retrieval"
            echo "document_text=" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Extract base URL (remove trailing /webhook/sonar if present)
          BASE_URL=$(echo "$WEBHOOK_URL" | sed 's|/webhook/sonar$||' | sed 's|/webhook$||' | sed 's|/$||')
          
          # Call the test attachment endpoint
          RESPONSE=$(curl -s -w "\n%{http_code}" -X GET "${BASE_URL}/webhook/test-attachment/${REPO_NAME}")
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          RESPONSE_BODY=$(echo "$RESPONSE" | sed '$d')
          
          echo "DEBUG: Test attachment HTTP Status: $HTTP_CODE"
          
          if [ "$HTTP_CODE" = "200" ]; then
            # Extract textContent from response
            TEXT_CONTENT=$(echo "$RESPONSE_BODY" | jq -r '.textContent // ""')
            
            if [ -n "$TEXT_CONTENT" ] && [ "$TEXT_CONTENT" != "null" ]; then
              echo "DEBUG: Successfully retrieved test attachment document"
              echo "document_text<<EOF" >> $GITHUB_OUTPUT
              echo "$TEXT_CONTENT" >> $GITHUB_OUTPUT
              echo "EOF" >> $GITHUB_OUTPUT
            else
              echo "WARNING: Empty textContent in response"
              echo "document_text=" >> $GITHUB_OUTPUT
            fi
          else
            echo "WARNING: Failed to retrieve test attachment (HTTP $HTTP_CODE)"
            echo "Response: $RESPONSE_BODY"
            echo "document_text=" >> $GITHUB_OUTPUT
          fi

      - name: Generate AI Analysis
        id: ai-analysis
        env:
          OLLAMA_API_KEY: ${{ secrets.OLLAMA_API_KEY }}
          OLLAMA_BASE_URL: ${{ secrets.OLLAMA_BASE_URL || 'https://ollama.com' }}
          OLLAMA_MODEL: ${{ secrets.OLLAMA_MODEL || 'gpt-oss:120b' }}
        run: |
          # Prepare the prompt for AI analysis
          DOCUMENT_TEXT="${{ steps.test-attachment.outputs.document_text }}"
          CODE_SAMPLES="${{ steps.code-extraction.outputs.code_samples }}"
          
          # Check if document text exists
          if [ -n "$DOCUMENT_TEXT" ] && [ "$DOCUMENT_TEXT" != "" ]; then
            DOCUMENT_SECTION="DOCUMENT DE TEST DU CANDIDAT:
          Ce document contient les instructions et le contexte du test à réaliser. Utilise-le pour évaluer l'avancement et la pertinence de la solution implémentée.
          
          $DOCUMENT_TEXT
          
          BASÉ SUR CE DOCUMENT, tu dois:
          1. Évaluer le degré d'avancement (0-100%) des objectifs du test
          2. Évaluer la qualité de l'implémentation par rapport aux attentes
          3. Analyser si le code répond aux exigences spécifiées"
          else
            DOCUMENT_SECTION="DOCUMENT DE TEST: Non disponible"
          fi
          
          cat > /tmp/prompt.txt << PROMPT_EOF
          Tu es un expert en analyse de code et qualité logicielle. Analyse le projet suivant basé sur les métriques SonarCloud, le document de test du candidat, ET le code source fourni, puis génère un rapport complet en français.
      
          INFORMATIONS DU PROJET:
          - Nom du projet: ${{ inputs.repo_name }}
          - Branche: ${{ inputs.branch_name }}
          - Fichiers analysés: ${{ steps.code-extraction.outputs.file_count }}
          - Lignes de code extraites: ${{ steps.code-extraction.outputs.total_lines }}
      
          $DOCUMENT_SECTION
      
          MÉTRIQUES SONARCLOUD:
          - Bugs: ${{ steps.sonar-metrics.outputs.bugs }}
          - Vulnérabilités: ${{ steps.sonar-metrics.outputs.vulnerabilities }}
          - Code smells: ${{ steps.sonar-metrics.outputs.code_smells }}
          - Couverture de code: ${{ steps.sonar-metrics.outputs.coverage }}%
          - Lignes de code totales: ${{ steps.sonar-metrics.outputs.lines_of_code }}
          
          CODE SOURCE À ANALYSER:
          $CODE_SAMPLES
          
          INSTRUCTIONS D'ANALYSE:
          1. Analyse le code source fourni pour identifier:
             - La qualité de l'architecture et de l'organisation du code
             - Les patterns de programmation utilisés (bonnes pratiques, anti-patterns)
             - La lisibilité et la maintenabilité du code
             - La gestion des erreurs et la robustesse
             - La sécurité du code
             - La conformité avec les exigences du document de test
          
          2. Compare le code avec les métriques SonarCloud pour valider ou nuancer les résultats
          
          3. Évalue si l'implémentation répond aux objectifs du document de test
      
          Réponds UNIQUEMENT au format JSON suivant avec l'analyse au format HTML simple (sans styles inline):
          {
            "score": [score de 0 à 100 basé sur la qualité du code, l'analyse du code source ET les métriques SonarCloud],
            "percentage_advancement": [pourcentage de 0 à 100 indiquant l'avancement par rapport au document de test],
            "analysis_html": "<div class=\"space-y-1\"><p class=\"text-electric-teal font-semibold\">Points forts:</p><div class=\"pl-4 space-y-0.5\"><p class=\"text-sm text-cool-gray\">Point fort 1</p><p class=\"text-sm text-cool-gray\">Point fort 2</p><p class=\"text-sm text-cool-gray\">Point fort 3</p></div></div><div class=\"space-y-1\"><p class=\"text-vibrant-orange font-semibold\">Points faibles:</p><div class=\"pl-4 space-y-0.5\"><p class=\"text-sm text-cool-gray\">Point faible 1</p><p class=\"text-sm text-cool-gray\">Point faible 2</p><p class=\"text-sm text-cool-gray\">Point faible 3</p></div></div><div class=\"space-y-1\"><p class=\"text-deep-blue font-semibold\">Recommandations:</p><div class=\"pl-4 space-y-0.5\"><p class=\"text-sm text-cool-gray\">Recommandation 1</p><p class=\"text-sm text-cool-gray\">Recommandation 2</p><p class=\"text-sm text-cool-gray\">Recommandation 3</p></div></div>"
          }
      
          IMPORTANT:
          - Utilise UNIQUEMENT les classes CSS suivantes: space-y-1, text-electric-teal, text-vibrant-orange, text-deep-blue, font-semibold, pl-4, space-y-0.5, text-sm, text-cool-gray
          - NE mets PAS de styles inline (pas de style="...")
          - Génère 3-5 points pour chaque section (Points forts, Points faibles, Recommandations)
          - Base ton analyse PRINCIPALEMENT sur le code source fourni, en complément des métriques SonarCloud
          - Cite des exemples spécifiques du code dans ton analyse (noms de fichiers, fonctions, patterns observés)
          - Si un document de test est disponible, vérifie que le code implémente les fonctionnalités demandées
          - Le "score" reflète une évaluation globale basée sur: la qualité du code observé (60%), les métriques SonarCloud (20%), et la conformité au document de test (20%)
          - Le "percentage_advancement" reflète le pourcentage de réalisation des objectifs du document de test basé sur l'analyse du code
          - Retourne UNIQUEMENT le JSON, sans texte supplémentaire ni markdown
          PROMPT_EOF
          
          PROMPT=$(cat /tmp/prompt.txt)
          ESCAPED_PROMPT=$(echo "$PROMPT" | jq -R -s .)
          
          cat > /tmp/payload.json << JSON_EOF
          {
            "model": "$OLLAMA_MODEL",
            "messages": [
              {
                "role": "user",
                "content": $ESCAPED_PROMPT
              }
            ],
            "temperature": 0.7,
            "max_tokens": 4000
          }
          JSON_EOF
          
          echo "DEBUG: Calling API at $OLLAMA_BASE_URL"
          echo "DEBUG: Model: $OLLAMA_MODEL"
          echo "DEBUG: Prompt size: ${#PROMPT} characters"
          
          # Call API with better error handling
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST "$OLLAMA_BASE_URL/v1/chat/completions" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $OLLAMA_API_KEY" \
            -d @/tmp/payload.json)
          
          HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
          RESPONSE_BODY=$(echo "$RESPONSE" | sed '$d')
          
          echo "DEBUG: HTTP Status: $HTTP_CODE"
          echo "DEBUG: Response: $RESPONSE_BODY" | head -c 500
          
          if [ "$HTTP_CODE" != "200" ]; then
            echo "ERROR: API returned HTTP $HTTP_CODE"
            echo "Full response:"
            echo "$RESPONSE_BODY"
            exit 1
          fi
          
          # Extract the AI response
          AI_RESPONSE=$(echo "$RESPONSE_BODY" | jq -r '.choices[0].message.content // empty')
          FINISH_REASON=$(echo "$RESPONSE_BODY" | jq -r '.choices[0].finish_reason // "unknown"')
          
          if [ -z "$AI_RESPONSE" ]; then
            echo "ERROR: No AI response extracted"
            echo "Full response body:"
            echo "$RESPONSE_BODY"
            exit 1
          fi
          
          echo "DEBUG: AI Response received (length: ${#AI_RESPONSE})"
          echo "DEBUG: Finish reason: $FINISH_REASON"
          
          # Check if response was truncated
          if [ "$FINISH_REASON" = "length" ]; then
            echo "WARNING: Response was truncated due to token limit"
          fi
          
          # Try to parse JSON response
          SCORE=$(echo "$AI_RESPONSE" | jq -r '.score // "N/A"' 2>/dev/null)
          PERCENTAGE_ADVANCEMENT=$(echo "$AI_RESPONSE" | jq -r '.percentage_advancement // "N/A"' 2>/dev/null)
          echo "$AI_RESPONSE" | jq -r '.analysis_html // "N/A"' 2>/dev/null > /tmp/analysis_html.txt
          ANALYSIS_HTML=$(cat /tmp/analysis_html.txt)
          
          # Fallback: if JSON parsing failed, set defaults
          if [ "$SCORE" = "N/A" ] || [ -z "$SCORE" ]; then
            echo "WARNING: Could not parse AI response as JSON, using defaults"
            echo "DEBUG: Raw AI response (first 1000 chars):"
            echo "$AI_RESPONSE" | head -c 1000
            echo ""
            
            EXTRACTED_SCORE=$(echo "$AI_RESPONSE" | grep -o '"score"[[:space:]]*:[[:space:]]*[0-9]*' | grep -o '[0-9]*' | head -1)
            if [ -n "$EXTRACTED_SCORE" ] && [ "$EXTRACTED_SCORE" -ge 0 ] && [ "$EXTRACTED_SCORE" -le 100 ]; then
              SCORE="$EXTRACTED_SCORE"
            else
              SCORE="50"
            fi
            
            EXTRACTED_PERCENTAGE=$(echo "$AI_RESPONSE" | grep -o '"percentage_advancement"[[:space:]]*:[[:space:]]*[0-9]*' | grep -o '[0-9]*' | head -1)
            if [ -n "$EXTRACTED_PERCENTAGE" ] && [ "$EXTRACTED_PERCENTAGE" -ge 0 ] && [ "$EXTRACTED_PERCENTAGE" -le 100 ]; then
              PERCENTAGE_ADVANCEMENT="$EXTRACTED_PERCENTAGE"
            else
              PERCENTAGE_ADVANCEMENT="0"
            fi
            
            ANALYSIS_HTML='<div class="space-y-1"><p class="text-vibrant-orange font-semibold">Points faibles:</p><div class="pl-4 space-y-0.5"><p class="text-sm text-cool-gray">La reponse AI a ete tronquee ou n a pas pu etre parsee correctement</p></div></div>'
          fi
          
          # Ensure percentage_advancement has a valid value
          if [ "$PERCENTAGE_ADVANCEMENT" = "N/A" ] || [ -z "$PERCENTAGE_ADVANCEMENT" ]; then
            PERCENTAGE_ADVANCEMENT="0"
          fi
          
          {
            echo "score=$SCORE"
            echo "percentage_advancement=$PERCENTAGE_ADVANCEMENT"
            echo "analysis_html<<EOF"
            echo "$ANALYSIS_HTML"
            echo "EOF"
          } >> $GITHUB_OUTPUT

      - name: Display AI Analysis Results
        run: |
          echo "=== RÉSULTATS DE L'ANALYSE IA ==="
          echo "Fichiers analysés: ${{ steps.code-extraction.outputs.file_count }}"
          echo "Lignes de code analysées: ${{ steps.code-extraction.outputs.total_lines }}"
          echo "Score (qualité technique): ${{ steps.ai-analysis.outputs.score }}/100"
          echo "Pourcentage d'avancement: ${{ steps.ai-analysis.outputs.percentage_advancement }}%"
          echo ""
          echo "=== RAPPORT HTML COMPLET ==="
          echo "${{ steps.ai-analysis.outputs.analysis_html }}"
          echo ""
          echo "=== FIN DU RAPPORT ==="